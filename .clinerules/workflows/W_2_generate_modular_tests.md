# Workflow: Generaci√≥n de Tests Modulares

## Descripci√≥n
Workflow para generar y mantener tests estructurados siguiendo el patr√≥n modular: tests unitarios para archivos individuales y tests de integraci√≥n para m√≥dulos completos. Utiliza el sistema autocode para detectar autom√°ticamente tests faltantes, generarlos usando pytest, y validar que todos funcionan correctamente.

## Resumen Visual

```mermaid
flowchart TD
    A[üéØ Solicitud de Tests] --> B[üîç uv run -m autocode.cli check-tests]
    B --> C{¬øHay tests faltantes?}
    C -->|S√≠| D[üìù Generar Tests Faltantes]
    C -->|No| H[‚úÖ Todos los tests existen]
    D --> E[üß™ Tests Unitarios]
    E --> F[üîó Tests de Integraci√≥n]
    F --> G[‚ñ∂Ô∏è Ejecutar Tests: pytest]
    G --> I{¬øTests pasan?}
    I -->|S√≠| J[üîç Verificar: autocode check-tests]
    I -->|No| K[üîß Corregir Tests]
    K --> G
    J --> L{¬øExit code 0?}
    L -->|S√≠| M[‚úÖ √âxito Completo]
    L -->|No| N[‚ö†Ô∏è Revisar pendientes]
```

El flujo comienza detectando autom√°ticamente qu√© tests necesitan crearse o actualizarse, genera tests modulares usando templates pytest, los ejecuta para verificar funcionalidad, y valida el resultado con el mismo comando inicial.

## Activaci√≥n del Workflow

### Cu√°ndo Usar
- Cuando se modifiquen archivos de c√≥digo fuente
- Al a√±adir nuevos m√≥dulos o funcionalidades
- Cuando el usuario solicite: "Genera/actualiza los tests modulares"
- Despu√©s de refactorings que requieran actualizar tests
- Para establecer cobertura de testing inicial en el proyecto

### Condiciones Previas
- Proyecto con configuraci√≥n uv (pyproject.toml)
- Pytest instalado como dependencia de desarrollo
- Estructura de c√≥digo fuente organizada en directorios
- Acceso al m√≥dulo autocode configurado con comando check-tests

## Inputs/Entradas
- Proyecto con c√≥digo fuente en directorio principal (ej. `vidi/`, `autocode/`, `tools/`)
- Configuraci√≥n uv para ejecutar autocode y pytest
- Estructura de directorios del proyecto
- Informaci√≥n de tests faltantes proporcionada por autocode

## Proceso

### Paso 1: Detecci√≥n Autom√°tica de Tests Faltantes
**EJECUTAR SIEMPRE COMO PRIMER PASO (Plan Mode y Act Mode)**
```bash
uv run -m autocode.cli check-tests
```

**Prop√≥sito**: Identificar autom√°ticamente qu√© tests est√°n faltantes, desactualizados o no funcionan correctamente.

**Salida esperada**: Lista de tests que requieren atenci√≥n, clasificados por tipo:
- `test_archivo.py` (tests unitarios para archivos individuales)
- `test_[modulo]_integration.py` (tests de integraci√≥n para m√≥dulos)
- Tests existentes que fallan o est√°n desactualizados

### Paso 2: An√°lisis de Resultados
- **Exit code 0**: No hay tests pendientes ‚Üí **Workflow completo**
- **Exit code 1**: Hay tests faltantes o fallando ‚Üí **Continuar con generaci√≥n**

### Paso 3: Generaci√≥n de Tests por Niveles

#### Nivel 1: Tests Unitarios (test_archivo.py)

**Proceso de an√°lisis**:
1. **Leer archivo fuente** completo
2. **Extraer funciones y clases** p√∫blicas
3. **Identificar dependencias** y puntos de mock
4. **Determinar casos de prueba** b√°sicos y edge cases

**Template para Tests Unitarios**:
```python
"""
Tests unitarios para [ruta/archivo.py]
"""

import pytest
from unittest.mock import Mock, patch, MagicMock
import sys
from pathlib import Path

# A√±adir el directorio ra√≠z al path para imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from [ruta.modulo] import [ClasePrincipal, funcion_principal]


class Test[ClasePrincipal]:
    """Tests unitarios para la clase [ClasePrincipal]"""
    
    def test_init(self):
        """Test inicializaci√≥n b√°sica"""
        # Arrange
        param1 = "valor_test"
        param2 = 123
        
        # Act
        instance = [ClasePrincipal](param1, param2)
        
        # Assert
        assert instance.param1 == param1
        assert instance.param2 == param2
    
    def test_metodo_principal_caso_exitoso(self):
        """Test m√©todo principal con caso exitoso"""
        # Arrange
        instance = [ClasePrincipal]("test", 100)
        input_data = {"key": "value"}
        
        # Act
        result = instance.metodo_principal(input_data)
        
        # Assert
        assert result is not None
        assert isinstance(result, expected_type)
    
    def test_metodo_principal_caso_error(self):
        """Test m√©todo principal con caso de error"""
        # Arrange
        instance = [ClasePrincipal]("test", 100)
        
        # Act & Assert
        with pytest.raises(ExpectedError):
            instance.metodo_principal(None)
    
    @patch('[ruta.modulo].dependencia_externa')
    def test_metodo_con_dependencia_externa(self, mock_dependencia):
        """Test m√©todo que usa dependencias externas"""
        # Arrange
        mock_dependencia.return_value = "mocked_result"
        instance = [ClasePrincipal]("test", 100)
        
        # Act
        result = instance.metodo_con_dependencia()
        
        # Assert
        assert result == "expected_result"
        mock_dependencia.assert_called_once()


class Test[FuncionPrincipal]:
    """Tests unitarios para funciones independientes"""
    
    def test_funcion_principal_caso_basico(self):
        """Test funci√≥n principal con entrada v√°lida"""
        # Arrange
        input_param = "test_input"
        expected_output = "expected_result"
        
        # Act
        result = funcion_principal(input_param)
        
        # Assert
        assert result == expected_output
    
    def test_funcion_principal_casos_edge(self):
        """Test funci√≥n principal con casos edge"""
        # Test con entrada vac√≠a
        assert funcion_principal("") == default_value
        
        # Test con entrada None
        with pytest.raises(ValueError):
            funcion_principal(None)
        
        # Test con entrada muy larga
        long_input = "x" * 1000
        result = funcion_principal(long_input)
        assert len(result) <= expected_max_length


# Fixtures para tests
@pytest.fixture
def sample_[clase_principal]():
    """Fixture para crear instancia de prueba"""
    return [ClasePrincipal]("test_param", 123)


@pytest.fixture
def sample_data():
    """Fixture para datos de prueba"""
    return {
        "key1": "value1",
        "key2": 456,
        "key3": ["item1", "item2"]
    }


# Tests de integraci√≥n b√°sica dentro del archivo
class TestIntegracion[NombreArchivo]:
    """Tests de integraci√≥n de componentes dentro del archivo"""
    
    def test_flujo_completo_basico(self, sample_data):
        """Test flujo completo usando m√∫ltiples componentes"""
        # Arrange
        instance = [ClasePrincipal]("integration_test", 999)
        
        # Act
        processed_data = funcion_principal(sample_data["key1"])
        result = instance.metodo_principal(processed_data)
        
        # Assert
        assert result is not None
        assert "expected_pattern" in str(result)
```

#### Nivel 2: Tests de Integraci√≥n (test_[modulo]_integration.py)

**Proceso de an√°lisis**:
1. **Identificar archivos** del m√≥dulo
2. **Mapear interacciones** entre componentes
3. **Definir flujos de trabajo** principales del m√≥dulo
4. **Identificar APIs p√∫blicas** del m√≥dulo

**Template para Tests de Integraci√≥n**:
```python
"""
Tests de integraci√≥n para el m√≥dulo [nombre_modulo]
"""

import pytest
from unittest.mock import Mock, patch, MagicMock
import sys
from pathlib import Path

# A√±adir el directorio ra√≠z al path para imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from [ruta.modulo] import [ComponenteA, ComponenteB, funcion_orchestradora]


class TestIntegracion[NombreModulo]:
    """Tests de integraci√≥n entre componentes del m√≥dulo"""
    
    def test_flujo_principal_completo(self):
        """Test del flujo de trabajo principal del m√≥dulo"""
        # Arrange
        componente_a = ComponenteA("config_test")
        componente_b = ComponenteB("param_test")
        
        # Act
        resultado_a = componente_a.procesar_entrada("input_data")
        resultado_final = componente_b.procesar_resultado(resultado_a)
        
        # Assert
        assert resultado_final is not None
        assert resultado_final.status == "success"
        assert "expected_content" in resultado_final.data
    
    def test_orquestacion_completa(self):
        """Test de orquestaci√≥n completa usando funci√≥n principal"""
        # Arrange
        config = {"param1": "value1", "param2": 123}
        input_data = {"test": "data"}
        
        # Act
        result = funcion_orchestradora(config, input_data)
        
        # Assert
        assert result.is_valid()
        assert result.components_used == ["ComponenteA", "ComponenteB"]
    
    def test_manejo_errores_entre_componentes(self):
        """Test manejo de errores en interacciones entre componentes"""
        # Arrange
        componente_a = ComponenteA("invalid_config")
        componente_b = ComponenteB("param_test")
        
        # Act & Assert
        with pytest.raises(IntegrationError):
            resultado_a = componente_a.procesar_entrada("invalid_data")
            componente_b.procesar_resultado(resultado_a)
    
    @patch('[ruta.modulo].dependencia_externa')
    def test_integracion_con_dependencias_externas(self, mock_external):
        """Test integraci√≥n con dependencias externas del m√≥dulo"""
        # Arrange
        mock_external.return_value = {"status": "ok", "data": "mocked"}
        
        # Act
        result = funcion_orchestradora({"use_external": True}, {"test": "input"})
        
        # Assert
        assert result.external_data == "mocked"
        mock_external.assert_called_once()


class TestAPIsPublicas[NombreModulo]:
    """Tests de las APIs p√∫blicas expuestas por el m√≥dulo"""
    
    def test_api_entrada_principal(self):
        """Test de la API de entrada principal del m√≥dulo"""
        # Arrange
        api_input = {
            "operation": "process",
            "data": {"key": "value"},
            "options": {"verbose": True}
        }
        
        # Act
        result = [modulo].public_api_function(api_input)
        
        # Assert
        assert result["status"] == "success"
        assert "result" in result
        assert result["metadata"]["processed_by"] == "[NombreModulo]"
    
    def test_configuracion_modulo(self):
        """Test configuraci√≥n y inicializaci√≥n del m√≥dulo"""
        # Arrange
        config = {
            "debug": True,
            "timeout": 30,
            "retries": 3
        }
        
        # Act
        module_instance = [modulo].initialize(config)
        
        # Assert
        assert module_instance.is_configured()
        assert module_instance.config.debug == True
        assert module_instance.config.timeout == 30


class TestRendimiento[NombreModulo]:
    """Tests de rendimiento y l√≠mites del m√≥dulo"""
    
    def test_rendimiento_carga_normal(self):
        """Test rendimiento con carga normal"""
        # Arrange
        normal_data = [{"id": i, "data": f"item_{i}"} for i in range(100)]
        
        # Act
        import time
        start = time.time()
        result = [modulo].process_batch(normal_data)
        duration = time.time() - start
        
        # Assert
        assert duration < 5.0  # Debe procesarse en menos de 5 segundos
        assert len(result) == 100
        assert all(item["processed"] for item in result)
    
    def test_limites_entrada_maxima(self):
        """Test l√≠mites con entrada m√°xima"""
        # Arrange
        max_data = [{"id": i, "data": f"item_{i}"} for i in range(1000)]
        
        # Act & Assert
        try:
            result = [modulo].process_batch(max_data)
            assert len(result) <= 1000
        except ResourceLimitError:
            # Es aceptable que tenga l√≠mites
            pass


# Fixtures para tests de integraci√≥n
@pytest.fixture
def configured_module():
    """Fixture para m√≥dulo configurado"""
    config = {
        "debug": False,
        "timeout": 10,
        "retries": 1
    }
    return [modulo].initialize(config)


@pytest.fixture
def sample_integration_data():
    """Fixture para datos de integraci√≥n"""
    return {
        "batch_data": [
            {"id": 1, "type": "test", "content": "sample1"},
            {"id": 2, "type": "test", "content": "sample2"}
        ],
        "metadata": {
            "source": "test_suite",
            "timestamp": "2025-01-01T00:00:00Z"
        }
    }
```

### Paso 4: Ejecuci√≥n de Tests Generados
```bash
pytest tests/ -v --tb=short
```

**Prop√≥sito**: Ejecutar todos los tests generados para verificar que funcionan correctamente.

**Criterios de √©xito**:
- Todos los tests pasan (exit code 0)
- No hay errores de sintaxis o imports
- Cobertura b√°sica de funcionalidad principal

### Paso 5: Correcci√≥n de Tests (si fallan)
Si pytest reporta errores:
1. **Analizar errores** espec√≠ficos reportados
2. **Corregir imports** o dependencias faltantes
3. **Ajustar mocks** o fixtures seg√∫n sea necesario
4. **Revisar l√≥gica** de tests si hay falsos positivos
5. **Volver a ejecutar** pytest hasta que pasen

### Paso 6: Verificaci√≥n Final
```bash
uv run -m autocode.cli check-tests
```

**Resultado esperado**:
- **Exit code 0**: ‚úÖ Todos los tests existen y pasan
- **Mensaje**: `‚úÖ All tests are complete and passing!`

## Outputs/Salidas
- Tests completos en estructura modular bajo `tests/`
- Tests unitarios `test_archivo.py` para cada archivo de c√≥digo
- Tests de integraci√≥n `test_[modulo]_integration.py` para cada m√≥dulo
- Todos los tests ejecut√°ndose exitosamente con pytest

## Criterios de √âxito

### Verificaci√≥n Autom√°tica
```bash
uv run -m autocode.cli check-tests
```
- **Exit code 0**: ‚úÖ √âxito completo
- **Exit code 1**: ‚ùå Tests faltantes o fallando

### Verificaci√≥n Manual con Pytest
```bash
pytest tests/ -v --tb=short --cov=vidi --cov=autocode
```
- **Todos los tests pasan**: ‚úÖ Funcionalidad correcta
- **Cobertura b√°sica**: ‚úÖ Componentes principales cubiertos

### Checklist Manual
- [ ] Existe test unitario para cada archivo de c√≥digo relevante
- [ ] Existe test de integraci√≥n para cada m√≥dulo
- [ ] Los tests cubren casos exitosos y casos de error
- [ ] Los mocks est√°n configurados correctamente
- [ ] Los tests son ejecutables y pasan
- [ ] La estructura de tests refleja la estructura del c√≥digo

## Estructura de Tests Target

```
tests/                                  # Directorio de tests del proyecto
‚îú‚îÄ‚îÄ test_main.py                       # Test del archivo main.py
‚îú‚îÄ‚îÄ [codigo_fuente]/                   # Tests del directorio de c√≥digo fuente
‚îÇ   ‚îú‚îÄ‚îÄ test_[codigo_fuente]_integration.py  # Tests de integraci√≥n del m√≥dulo principal
‚îÇ   ‚îú‚îÄ‚îÄ [modulo1]/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_[modulo1]_integration.py    # Tests de integraci√≥n del m√≥dulo1
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_archivo1.py                 # Test unitario de archivo1.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ [submodulo]/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ test_[submodulo]_integration.py  # Tests de integraci√≥n del subm√≥dulo
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ test_archivo2.py             # Test unitario de archivo2.py
‚îÇ   ‚îî‚îÄ‚îÄ [modulo2]/
‚îÇ       ‚îú‚îÄ‚îÄ test_[modulo2]_integration.py    # Tests de integraci√≥n del m√≥dulo2
‚îÇ       ‚îî‚îÄ‚îÄ test_archivo3.py                 # Test unitario de archivo3.py
‚îú‚îÄ‚îÄ conftest.py                        # Fixtures y configuraci√≥n global pytest
‚îî‚îÄ‚îÄ pytest.ini                        # Configuraci√≥n pytest
```

### Regla de Mapeo para Tests

**C√≥digo**: `[codigo_fuente]/[ruta]/[archivo].[ext]`  
**Test Unitario**: `tests/[codigo_fuente]/[ruta]/test_[archivo].py`  
**Test Integraci√≥n**: `tests/[codigo_fuente]/[ruta]/test_[modulo]_integration.py`

**Ejemplos concretos**:
- `vidi/inference/engine.py` ‚Üí `tests/vidi/inference/test_engine.py`
- `vidi/inference/` ‚Üí `tests/vidi/inference/test_inference_integration.py`
- `autocode/core/doc_checker.py` ‚Üí `tests/autocode/core/test_doc_checker.py`
- `autocode/core/` ‚Üí `tests/autocode/core/test_core_integration.py`

### Tipos de Tests por Componente

1. **Tests Unitarios** (`test_archivo.py`):
   - Testean clases y funciones individuales
   - Usan mocks para todas las dependencias externas
   - Verifican casos exitosos y casos de error
   - Incluyen casos edge y validaci√≥n de entrada

2. **Tests de Integraci√≥n** (`test_[modulo]_integration.py`):
   - Testean interacci√≥n entre componentes del m√≥dulo
   - Verifican flujos de trabajo completos
   - Testean APIs p√∫blicas del m√≥dulo
   - Incluyen tests de rendimiento b√°sico

## Comandos de Referencia

### Comando Principal
```bash
# Detectar tests faltantes y verificar existentes
uv run -m autocode.cli check-tests

# Verificar resultado (debe devolver exit code 0)
echo $?
```

### Comandos de Testing
```bash
# Ejecutar todos los tests
pytest tests/ -v

# Ejecutar tests con cobertura
pytest tests/ --cov=vidi --cov=autocode --cov-report=html

# Ejecutar tests de un m√≥dulo espec√≠fico
pytest tests/vidi/inference/ -v

# Ejecutar solo tests unitarios
pytest tests/ -k "not integration" -v

# Ejecutar solo tests de integraci√≥n
pytest tests/ -k "integration" -v
```

### Comandos de Exploraci√≥n (para an√°lisis inicial)
```bash
# Explorar estructura del proyecto
list_files [directorio_proyecto]/ --recursive

# Leer archivos para extraer funciones y clases
read_file [archivo_fuente].py
```

## Necesidades de Implementaci√≥n en Autocode

### Comando CLI Requerido
```bash
uv run -m autocode.cli check-tests
```

### Funcionalidades Necesarias en Autocode

1. **TestChecker** (similar a DocChecker):
   - Detectar archivos sin tests correspondientes
   - Verificar que tests existentes est√°n actualizados
   - Ejecutar tests y reportar fallos

2. **Configuraci√≥n en autocode_config.yml**:
   ```yaml
   tests:
     enabled: true
     directories:
       - "vidi/"
       - "autocode/"
       - "tools/"
     exclude:
       - "__pycache__/"
       - "*.pyc"
       - "__init__.py"
     test_frameworks:
       - "pytest"
   ```

3. **Integraci√≥n con pytest**:
   - Ejecutar pytest program√°ticamente
   - Capturar resultados y errores
   - Reportar cobertura b√°sica

## Recordatorios para Cline

### Principios Fundamentales
1. **OBLIGATORIO**: Ejecutar `uv run -m autocode.cli check-tests` como primer paso, incluso en plan mode, antes de cualquier an√°lisis
2. **SIEMPRE** generar tests unitarios antes que tests de integraci√≥n
3. **SIEMPRE** leer c√≥digo fuente completo antes de generar tests
4. **SIEMPRE** usar los templates definidos consistentemente
5. **SIEMPRE** ejecutar pytest despu√©s de generar tests
6. **SIEMPRE** verificar con autocode check-tests al final
7. **NUNCA** generar tests para archivos __init__.py

### Flujo de Trabajo Obligatorio
1. **Detectar necesidades (PRIMER PASO OBLIGATORIO)**: `uv run -m autocode.cli check-tests`
2. **Analizar estructura** del c√≥digo con list_files
3. **Generar tests unitarios** para archivos individuales
4. **Generar tests de integraci√≥n** para m√≥dulos
5. **Ejecutar tests**: `pytest tests/ -v`
6. **Corregir errores** si hay fallos
7. **Verificar resultado**: `uv run -m autocode.cli check-tests`
8. **Confirmar exit code 0**

### Criterios de Calidad para Tests
- **Cobertura**: Funciones y clases p√∫blicas principales cubiertas
- **Casos**: Incluir casos exitosos, errores y edge cases
- **Mocks**: Usar mocks apropiados para dependencias externas
- **Claridad**: Tests legibles y bien documentados
- **Mantenibilidad**: Tests f√°ciles de actualizar cuando cambie el c√≥digo
- **Velocidad**: Tests unitarios r√°pidos, integraci√≥n moderada

### Patrones de Testing por Tipo de C√≥digo

#### APIs y Servicios
- **Mock** llamadas HTTP y bases de datos
- **Test** casos de √©xito y c√≥digos de error
- **Validar** serializaci√≥n/deserializaci√≥n

#### Procesamiento de Datos
- **Test** con datos v√°lidos e inv√°lidos
- **Verificar** transformaciones y filtros
- **Validar** rendimiento con datasets grandes

#### Clases y M√≥dulos
- **Test** inicializaci√≥n y configuraci√≥n
- **Mock** dependencias externas
- **Verificar** interacciones entre m√©todos

#### Funciones Utilitarias
- **Test** casos edge exhaustivos
- **Verificar** tipos de entrada y salida
- **Validar** comportamiento con None y vac√≠os

### Manejo de Errores
- **Tests que fallan**: Analizar error espec√≠fico y corregir
- **Imports faltantes**: Verificar rutas y dependencias
- **Mocks incorrectos**: Ajustar seg√∫n interfaz real
- **Fixtures complejas**: Simplificar y modularizar

## Notas Importantes
- Este workflow requiere implementar el comando `check-tests` en autocode
- Los tests generados usan pytest como framework principal
- La estructura de tests refleja exactamente la estructura del c√≥digo
- Se enfoca en cobertura funcional b√°sica m√°s que cobertura exhaustiva
- Los tests de integraci√≥n verifican flujos de trabajo reales del m√≥dulo
- La validaci√≥n final usa el mismo comando que la detecci√≥n inicial
- Compatible con proyectos que tengan configuraci√≥n uv (pyproject.toml)
- Pytest debe estar instalado como dependencia de desarrollo
